---
title: "Sesion 08"
author: "Eduardo Martínez"
date: "2025-03-15"
output: html_document
---

```{r}
library(jsonlite)
```

```{r}
datos <- read_json("datos_prueba.json")
```

```{r}
datos |> class()
```

```{r}
datos |> str()
```

```{r}
datos |> dim()
```

```{r}
datos |> head(n = 2)
```

```{r}
datos_df <- read_json("datos_prueba.json", simplifyVector = TRUE)
```

```{r}
datos_df |> class()
```

```{r}
datos_df |> str()
```

```{r}
datos_df |> head()
```

```{r}
datos_df |> dim()
```

```{r}
datos_df |> dplyr::select(where(is.list))
```

```{r}
datos_df$F_liv[1]
```

```{r}
datos_df |> head(n=1) |> jsonlite::toJSON(pretty=TRUE)
```

## Introducción a la limpieza de texto

```{r}
mi_string <- "Ejemplo de STRING, con caraceteres varios (12, 15 y 10.2)?!"

mi_string
```

```{r}
(string_en_minuscula <- tolower(mi_string))
```

```{r}
otro_string <- "Wow, tengo más que decir!!"
paste(mi_string, otro_string, sep = " ")
```

```{r}
paste(mi_string, otro_string, sep = "@@@@")
```

```{r}
paste(mi_string, otro_string)
```

```{r}
paste0(mi_string, otro_string)
```

```{r}
mi_string <- "Ejemplo de STRING,      con caraceteres varios (12, 15 y 10.2)?!"

mi_string
```

```{r}
stringr::str_split(string = mi_string, pattern = " ")
```

```{r}
stringr::str_split(string = mi_string, pattern = boundary("word"))
```


```{r}
stringr::str_count(string = mi_string, pattern = " ")
```

```{r}
stringr::str_count(string = mi_string, pattern = boundary("word"))
```

```{r}
fruits <- c(
  "apples and oranges and pears and bananas",
  "pineapples and mangos and guavas"
)
```

```{r}
fruits |> stringr::str_split(pattern = " and ")
```


```{r}
fruits |> stringr::str_split(pattern = " and ", simplify = TRUE)
```

```{r}
fruits |> stringr::str_split(pattern = " and ", n = 3)
```

```{r}
fruits |> stringr::str_split(pattern = " and ", n = 2)
```

```{r}
fruits |> stringr::str_split(pattern = " and ", n = 5)
```

```{r}
fruits |> stringr::str_split_fixed(pattern = " and ", n = 3)
```

```{r}
fruits |> stringr::str_split_fixed(pattern =" and ", n = 5)
```

```{r}
fruits |> stringr::str_split_i(pattern = " and ", i = 1)
```

```{r}
fruits |> stringr::str_split_i(pattern = " and ", i = 4)
```

```{r}
fruits |> stringr::str_split_i(pattern = " and ", i = 2)
```

```{r}
fruits |> stringr::str_split_i(pattern = " and ", i = -1)
```

```{r}
fruits |> stringr::str_split_i(pattern = " and ", i = -2)
```


```{r}
infierno_de_i <- c("istanbul", "İzmar", "Istanbul", "izmar", "\u0130")
infierno_de_i
```
```{r}
stringr::str_detect(infierno_de_i, pattern = coll("i", TRUE))
```

coll: Función se utiliza para "collation" (algo así como cotejo o compaginación), que es una forma de comparar strings teniendo en cuenta reglas específicas de la configuración regional (por ejemplo, sensibilidad a mayúsculas y minúsculas, orden de caracteres, etc.).

TRUE: Este argumento especifica que la búsqueda debe ser sensible a mayúsculas y minúsculas. Si fuera FALSE, la búsqueda sería insensible a mayúsculas y minúsculas.

```{r}
stringr::str_detect(infierno_de_i, coll("i", TRUE, locale = "tr"))
```

locale = "tr", especifica la configuración regional que se utilizará para la collation. La configuración regional "tr" se refiere al turco. En turco, la letra "i" tiene un comportamiento especial en cuanto a la sensibilidad a mayúsculas y minúsculas (por ejemplo, la versión en mayúscula de "i" es "İ", y la versión en minúscula de "I" es "ı").

```{r}
stringr::str_detect(infierno_de_i, fixed("i", TRUE))
```

Con fixed se especifica que el patrón debe tratarse como un string fijo (i.e. no como una expresión regular). Esto significa que los caracteres especiales en el patrón se interpretan literalmente, no como metacaracteres de regex.

```{r}
mi_string
```

```{r}
str_split(mi_string, pattern = "!")
```

```{r}
str_split(mi_string, pattern = "!")[[1]]
```

```{r}
mi_string_en_vector <- str_split(mi_string, pattern = "!")[[1]]
```


```{r}
grep(pattern = "\\?", x = mi_string_en_vector)
```

```{r}
stringr::str_replace_all(mi_string, "e","@@")
```

```{r}
stringr::str_extract_all(mi_string,"[0-9]+")
```
```{r}
stringr::str_extract_all(mi_string,"[?]+")
```


```{r}
str_extract_all(mi_string, "[a-z]+")
```

```{r}
str_extract_all(mi_string, regex("[a-z]+", TRUE))
```



```{r}
mi_vector <- c("123 grapes", "apples x4", "bag of flour",
               "kiwi and lime", "Bag of sugar", "milk x2")
```

```{r}
str_extract(mi_vector, "\\d")
```

```{r}
str_extract_all(mi_vector, "\\d")
```

```{r}
str_extract(mi_vector, "[a-z]+")
```

```{r}
str_extract(mi_vector, "[a-z]{1,4}")
```

```{r}
str_extract(mi_vector, "[a-z]{1,3}")
```

```{r}
str_extract(mi_vector, "[a-z]{1,8}")
```

```{r}
str_extract(mi_vector, "\\b[a-z]+\\b")
```

Es un boundary (más en específico, un word boundary); hace que el match ocurra al inicio o al final de una palabra

```{r}
str_extract(mi_vector, "\\b[a-z]+")
```

```{r}
str_extract(mi_vector, "[a-z]+\\b")
```

```{r}
str_extract(mi_vector, regex("[a-z]+\\b", TRUE))
```

```{r}
str_extract(mi_vector, "([a-z]+) of ([a-z]+)")
```

```{r}
str_extract(mi_vector, "([a-z]+) of ([a-z]+)", group = 1)
```

```{r}
str_extract(mi_vector, "([a-z]+) of ([a-z]+)", group = 2)
```


```{r}
mi_texto <- read.delim("ObamaSpeech.txt", header = FALSE)
```

```{r}
mi_texto |> str()
```

```{r}
mi_texto[1,1] |> corpus() |> summary()
```

```{r}
mi_texto[1,1] |> quanteda::tokens() |>
    dfm()
```

```{r}
mi_texto[17,1]
```

```{r}
head(stopwords("en"), 20)
```

```{r}
head(stopwords("ru"), 10)
```

```{r}
head(stopwords("it"), 10)
```

```{r}
head(stopwords("es"), 10)
```


```{r}
mi_texto[17,1] |> quanteda::tokens() |> 
  tokens_remove(stopwords("en")) |>
  dfm()
```


```{r}
primera_frase <- "This is $10 in 999 different ways,\n up and down; left and right!"
segunda_frase <- "@koheiw7 working: on #quanteda 2day\t4ever, http://textasdata.com?page=123."
```


```{r}
texto_completo <- c(text1 = primera_frase,
                    text2 = segunda_frase,
                    text3 = mi_texto[17,1],
                    text4 = mi_texto[27,1],
                    text5 = mi_texto[37,1],
                    text6 = mi_texto[47,1],
                    text7 = mi_texto[57,1],
                    text8 = billboard::lyrics[5,"lyrics"])
```

```{r}
texto_completo |> quanteda::tokens()
```


```{r}
texto_completo |> quanteda::tokens () |>
  tokens_remove(stopwords("en")) |>
  dfm() |> textplot_wordcloud(min_count = 2)
```

```{r}
texto_completo |> quanteda::tokens(remove_numbers = TRUE,
                                   remove_punct = TRUE,
                                   remove_separators = TRUE) |>
  dfm() |> textplot_wordcloud()
```

```{r}
texto_completo |> quanteda::tokens(remove_numbers = TRUE,
                                   remove_punct = TRUE,
                                   remove_separators = TRUE) |>
  tokens_remove(stopwords("en")) |>
  dfm() |> textplot_wordcloud()
```
