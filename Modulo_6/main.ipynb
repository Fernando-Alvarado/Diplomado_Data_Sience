{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fcd098",
   "metadata": {},
   "source": [
    "# Modulo 6, reduccion de la dimension \n",
    "\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "+ t-SNE es un algoritmo que se utiliza para visualizar datos de alta dimensión. Debido a que no podemos visualizar nada que tenga más de dos (o quizás tres) dimensiones, t-SNE lo hace reduciendo la dimensión de los datos, mientras preserva la estructura de los mismos tanto como sea posible.\n",
    "\n",
    "+ Si comparamos t-SNE con PCA, vemos que PCA intenta preservar la información en los datos (a través de la varianza), mientras que t-SNE intentaría preservar las distancias relativas y la estructura de agrupamiento en los datos, dando menor importancia a la información total preservada.\n",
    "\n",
    "+ t-SNE es un algoritmo no lineal de reducción de dimensión, que representa cada objeto de alta dimensión por un punto bidimensional o tridimensional, de tal manera que los objetos similares se plasman en puntos cercanos y los objetos diferentes en puntos distantes.\n",
    "\n",
    "+ La idea central detrás de t-SNE es mapear puntos de datos de alta dimensión a un espacio de menor dimensión, típicamente dos o tres dimensiones, de una manera que preserve las relaciones locales entre los puntos.\n",
    "\n",
    "+ Matemáticamente, busca minimizar la divergencia entre dos distribuciones: una distribución que mide similitudes por pares de los objetos de entrada (datos en la dimensión original) y una distribución que mide similitudes por pares de los correspondientes puntos en baja dimensión.\n",
    "\n",
    "+ Entonces, minimiza la diferencia entre estas dos distribuciones utilizando una técnica llamada descenso de gradiente (minimiza la divergencia Kullback-Leibler entre las distribuciones).\n",
    "  \n",
    "+ Este proceso permite que t-SNE capture eficazmente la estructura local de los datos, lo que lo hace particularmente útil para visualizar conjuntos de datos complejos y descubrir patrones interesantes.\n",
    "\n",
    "\n",
    "Los pasos del algortimo son: \n",
    "\n",
    "+ Calcular similitudes por pares\n",
    "\n",
    "+ Construir distribuciones de probabilidad a partir de las similitudes anteriores\n",
    "\n",
    "+ Minimizar la divergencia de Kullback-Leibler: utilizar la técnica de descenso de gradiente para encontrar una representación de baja dimensión que minimice la divergencia entre las dos distribuciones.\n",
    "\n",
    "\n",
    "\n",
    "### UMAP: Uniform Manifold Approximation and Projection \n",
    "\n",
    "(Se basa en geometri rimmaniana)\n",
    "\n",
    "+ Uniform Manifold Approximation and Projection (UMAP) es una potente técnica de reducción de dimensión que ha ganado una gran aceptación en los campos del aprendizaje automático y la visualización de datos.\n",
    "\n",
    "+ UMAP está basada en sólidos fundamentos matemáticos, como la geometría de Riemann y la topología algebraica.\n",
    "\n",
    "+ Es una técnica de aprendizaje no supervisado que tiene como objetivo reducir la dimensionalidad de los datos y, al mismo tiempo, preservar su estructura topológica.\n",
    "\n",
    "+ Es particularmente útil para visualizar conjuntos de datos de alta dimensión en un espacio de baja dimensión, generalmente de dos o tres dimensiones.\n",
    "\n",
    "+ UMAP se compara a menudo con t-SNE (t-distributed Stochastic Neighbor Embedding) debido a su aplicación similar en la visualización de datos, pero ofrece varias ventajas, incluida una mejor preservación de la estructura global de los datos y tiempos de cálculo más rápidos.\n",
    "\n",
    "\n",
    "+ A pesar de las intimidantes matemáticas de sus fundamentos, la intuición detrás de los principios básicos de UMAP, son en realidad bastante simples.\n",
    "\n",
    "+ Esencialmente, construye un gráfico ponderado a partir de los datos de alta dimensión, donde la fuerza de los bordes representa qué tan “cerca” está un punto dado de otro. Posteriormente, proyecta este gráfico hacia una dimensionalidad menor.\n",
    "\n",
    "\n",
    "#### El algoritmo UMAP se puede dividir en dos fases principales: construcción de una representación topológica difusa y optimización de la proyección de baja dimensión.\n",
    "\n",
    "  **Construcción de la representación topológica difusa**\n",
    "  \n",
    "+ Búsqueda del vecino más cercano: UMAP comienza por encontrar los vecinos más cercanos para cada punto en los datos. Esto se hace normalmente utilizando algoritmos de vecino más cercano aproximado para acelerar el proceso.\n",
    "\n",
    "+ Conjunto simple difuso: se construye un conjunto simple difuso a partir de los vecinos más cercanos. Este conjunto captura la conectividad local de los puntos de datos.\n",
    "   \n",
    "+ Intensidades de pertenencia difusa: las intensidades de pertenencia se asignan a los bordes del conjunto simple, lo que representa la probabilidad de que dos puntos estén conectados.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2370e",
   "metadata": {},
   "source": [
    "## Resumen paquete de Tidymodels\n",
    "\n",
    "+ rsample: Prepara los datos para métodos supervisados y realiza remuestreos (bootstraping y validación cruzada)\n",
    "+ recipes: Preprocesa datos y facilita la ingeniería de selección de características (features)\n",
    "+ parsnip: Apoya la definición y el ajuste de modelos\n",
    "+ broom: Apoya el resumen de los modelos ajustados\n",
    "+ dials y tune: Responsables del afinamiento o exploración (tuning) de los parámetros de los modelos\n",
    "+ yardstick: Evalúa el rendimiento de los modelos y su validación\n",
    "+ workflows: Organiza el flujo de modelamiento (pipeline).\n",
    "+ rsample. Tiene funciones para crear remuestras, a través de procedimientos de bootstrap y validación cruzada, de un conjunto de datos que pueden usarse para evaluar modelos o estimar la distribución de muestreo de alguna estadística.\n",
    "+ recipes. Consiste en uno o más pasos de manipulación y análisis de datos. Los parámetros estadísticos para los pasos pueden estimarse a partir de un conjunto de datos inicial y luego aplicarse a otros conjuntos de datos. Está hecha de tal modo que difiere los cálculos.\n",
    "+ parsnip. Una interfaz para modelar que permite, mediante un llamado de parámetros unificado, probar una gran gama de modelos sin atorarse en las minucias sintácticas del subyacente paquete. Es el sucesor de la librería caret. También está hecha de tal modo que difiere los cálculos. Tiene un conjunto de librerías opcionales asociadas.\n",
    "+ broom. Convierte los objetos de análisis estadístico (fits) de R y los organiza en tres tipos de información mediante tres comandos genéricos: tidy(), que resume los hallazgos estadísticos de un modelo, como los coeficientes de una regresión; augment(), que agrega columnas a los datos originales con las predicciones, residuos y asignaciones de cluster; y glance(), que proporciona un resumen de estadísticas a nivel del modelo.\n",
    "+ dials. Contiene herramientas para crear y administrar valores de parámetros y está diseñado para integrarse bien con el paquete parsnip.\n",
    "+ tune. Contiene funciones y clases que se utilizan junto con otros paquetes tidymodels para encontrar valores razonables de los parámetros, por ejemplo, de regularización, métodos de procesamiento previo y pasos de procesamiento posterior.\n",
    "+ yardstick. Herramienta tidy para cuantificar qué tan bien se ajusta el modelo a un conjunto de datos: matrices de confusión, resúmenes de curvas de probabilidad y métricas.\n",
    "+ workflow. Es el componente fundamental que permite conectar el preprocesamiento de datos y el modelado predictivo. Permite ajustar y realizar predicciones con un simple llamado fit/predict, sobre un objeto único que guarda todo el preprocesamiento realizado.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
